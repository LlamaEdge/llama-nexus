name: Test Server

on:
  push:
    branches:
      - dev
      - main
      - release-*
      - feat-*
      - ci-*
      - refactor-*
      - fix-*
      - test-*
    paths:
      - ".github/workflows/test_server.yml"
      - "**/Cargo.toml"
      - "**/*.rs"
      - "**/*.sh"
      - "**/.cargo/config.toml"
      - "tests/*.hurl"
  pull_request:
    branches:
      - dev
      - main
    types: [opened, synchronize, reopened]
    paths:
      - ".github/workflows/**"
      - "**/Cargo.toml"
      - "**/*.rs"
      - "**/*.sh"
      - "tests/*.hurl"

jobs:
  test-api-server-ubuntu:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        wasmedge_version: [0.14.1]
        ggml_version: [b4762]
    steps:
      - name: Clone project
        id: checkout
        uses: actions/checkout@v3

      - name: Install Rust-nightly
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: nightly
          target: wasm32-wasip1
          components: rustfmt, clippy

      - name: Install Rust-stable
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable
          target: wasm32-wasip1

      - name: Install WasmEdge
        run: |
          curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v ${{ matrix.wasmedge_version }} --ggmlbn=${{ matrix.ggml_version }}
          ls -al $HOME/.wasmedge/bin

      - name: Install Hurl
        run: |
          curl --location --remote-name https://github.com/Orange-OpenSource/hurl/releases/download/5.0.1/hurl_5.0.1_amd64.deb
          sudo apt update && sudo apt install ./hurl_5.0.1_amd64.deb

      - name: Build LlamaEdge-Q
        run: |
          cargo build --release
          cp target/release/LlamaEdge-Q ./LlamaEdge-Q

      - name: Build llama-api-server.wasm
        env:
          RUSTFLAGS: "--cfg wasmedge --cfg tokio_unstable"
        run: |
          git clone -b refactor-update-serverinfo https://github.com/LlamaEdge/LlamaEdge.git
          cd LlamaEdge
          cargo build --release
          cp target/wasm32-wasip1/release/llama-api-server.wasm ../llama-api-server.wasm
          cd -

      - name: Start llama-api-server for testing chat completions
        run: |
          curl -LO https://huggingface.co/second-state/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q2_K.gguf
          nohup $HOME/.wasmedge/bin/wasmedge --dir .:. --nn-preload default:GGML:AUTO:Llama-3.2-1B-Instruct-Q2_K.gguf llama-api-server.wasm --model-name Llama-3.2-1B --prompt-template llama-3-chat --ctx-size 4096 --socket-addr 0.0.0.0:8080 > ./start-llamaedge.log 2>&1 &
          sleep 15
          cat start-llamaedge.log

      - name: Start LlamaEdge-Q and register llama-api-server
        run: |
          nohup ./LlamaEdge-Q > ./start-Q.log 2>&1 &
          curl --location 'http://localhost:9068/admin/servers/register' --header 'Content-Type: application/json' --data '{"url": "http://localhost:10086","kind": "chat"}
          sleep 5
          cat start-Q.log

      - name: Run test_chat.hurl
        run: |
          hurl --test --jobs 1 ./tests/test_chat.hurl

      - name: Stop LlamaEdge-Q
        run: |
          pkill -f LlamaEdge-Q

      - name: Stop llama-api-server for testing chat completions
        run: |
          pkill -f wasmedge

